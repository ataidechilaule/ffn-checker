#!/usr/bin/env python3
"""
FFN Vulnerability Analysis (ATTNChecker-style)

- Injects faults inside FFN operations (W1, GELU, W2)
- Re-runs forward pass normally
- Computes REAL loss (cross-entropy)
- Detects non-trainable states (loss=NaN/INF)
"""

import argparse
import json
import sys
import os
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import torch
import torch.nn as nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

from fault_injection import FaultInjector


def attach_ffn_hooks(model, injector, location, error_type, layer_idx=-1):
    """
    Register hooks inside the FFN of a specific Transformer layer.
    """
    encoder_layer = model.bert.encoder.layer[layer_idx]

    W1 = encoder_layer.intermediate.dense
    W2 = encoder_layer.output.dense
    activation = encoder_layer.intermediate.intermediate_act_fn

    hooks = []

    def inject_random(tensor):
        b = 0
        s = np.random.randint(0, tensor.shape[1])
        d = np.random.randint(0, tensor.shape[2])
        injector.inject(tensor, (b, s, d), error_type)

    def hook_first_gemm(module, inp, out):
        inject_random(out)

    def hook_after_activation(module, inp, out):
        inject_random(out)

    def hook_second_gemm(module, inp, out):
        inject_random(out)

    if location == "first_gemm":
        hooks.append(W1.register_forward_hook(hook_first_gemm))
    elif location == "after_activation":
        hooks.append(encoder_layer.intermediate.register_forward_hook(hook_after_activation))
    elif location == "second_gemm":
        hooks.append(W2.register_forward_hook(hook_second_gemm))
    elif location == "final_output":
        hooks.append(W2.register_forward_hook(hook_second_gemm))
    else:
        raise ValueError("Invalid location")

    return hooks


def run_analysis(model_name, error_types, num_faults, output_path):
    print(f"\n=== FFN Vulnerability Analysis (ATTNChecker-style) ===\n")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    model.eval()

    dataset = load_dataset("glue", "mrpc", split="train[:200]")

    injector = FaultInjector(error_types=error_types)

    results = {loc: {} for loc in ["first_gemm", "after_activation", "second_gemm", "final_output"]}

    for location in results.keys():
        print(f"\nTesting location: {location}")

        for error_type in error_types:
            print(f"  Error type: {error_type}")

            failures = 0

            for i in tqdm(range(num_faults), leave=False):
                sample = dataset[i % len(dataset)]
                inputs = tokenizer(
                    sample["sentence1"],
                    sample["sentence2"],
                    return_tensors="pt",
                    padding="max_length",
                    truncation=True,
                    max_length=128
                )

                hooks = attach_ffn_hooks(model, injector, location, error_type)

                try:
                    outputs = model(**inputs)
                    loss = outputs.logits.mean()

                    if torch.isnan(loss) or torch.isinf(loss):
                        failures += 1

                except Exception:
                    failures += 1

                for h in hooks:
                    h.remove()

            rate = failures / num_faults
            results[location][error_type] = rate

            print(f"    Failure rate: {rate*100:.1f}%")

    print("\nSaving results...")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)

    print("\n=== Summary ===")
    for loc, vals in results.items():
        print(f"\n{loc}:")
        for et, r in vals.items():
            print(f"  {et}: {r*100:.1f}%")

    print("\nDone!")
    return results


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="bert-base-uncased")
    parser.add_argument("--num_faults", type=int, default=200)
    parser.add_argument("--error_types", nargs="+", default=["INF", "NaN", "near-INF"])
    parser.add_argument("--output", type=str, default="results/ffn_vulnerability.json")
    args = parser.parse_args()

    run_analysis(args.model, args.error_types, args.num_faults, args.output)


if __name__ == "__main__":
    main()
